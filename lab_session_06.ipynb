{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). On JupyterLab, you may want to hit the \"Validate\" button as well.\n",
    "\n",
    "Caution: do not mess with the notebook's metadata; do not change a pre-existing cell's type; do not copy pre-existing cells (add new ones with the + button instead). This will break autograding; you will get a 0; you are warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border: none;\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "  <tr>\n",
    "    <td><img src=\"https://www.planetegrandesecoles.com/wp-content/uploads/2021/07/Identite%CC%81-visuelle-Plane%CC%80te-BAC-8-600x398.png\" style=\"float: left; width: 100%\" />\n",
    "</td>\n",
    "    <td><a style=\"font-size: 3em; text-align: center; vertical-align: middle;\" href=\"https://moodle.polytechnique.fr/course/view.php?id=17527\">[CSE204-2023] - Introduction to Machine Learning</a>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5YuK2f4u_CzI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ce6a3aaca74dbb2d3deda1c34cb7eb",
     "grade": false,
     "grade_id": "cell-d65e9154014fb110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a style=\"font-size: 3em;\">Lab Session 6: Building a Neural Network From Scratch - feedforward</a>\n",
    "\n",
    "J.B. Scoggins - Adrien Ehrhardt - Jesse Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "rWMgDSTC_CzL",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33024e0a6f971c4ccf59b08d06e9973f",
     "grade": false,
     "grade_id": "cell-6b9fe28aa4a433c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab you will learn to create a simple multilayer perceptron (MLP) neural network from scratch using only the Numpy package for matrix-vector operations.  In order to train the network on several datasets, you will need to implement the back-propagation algorithm with gradient descent (in lab 07!). Before getting started, let's review the notations we will use in this lab and recall the stochastic gradient descent algorithm.\n",
    "\n",
    "## Notation\n",
    "\n",
    "We will consider simple feed-forward networks that can be described by the following recursive relationship,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&z^l = a^{l-1} W^l + b^l,\\\\\n",
    "&a^l = \\sigma^l(z^l),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a^l$ is the output (activation) of layer $l$ which is a nonlinear function $\\sigma^l$ of a linear transformation of the previous layer's output.  The linear transformation is performed using the weight matrix $W^l$ and bias vector $b^l$ associated with the layer $l$.  We will denote the last layer in the network with a capital $L$ superscript.  The recursion is stopped by setting $a^0 = x$, where $x$ is the input vector to our network.  Note that in the recursive expressions above, we implicitly assume that our input/output vectors are row vectors.  The reason for this will be apparent later.\n",
    "\n",
    "Taking this notation into account, we see that a network with $L-1$ hidden layers is fully expressed by its $L$ weight matrices, bias vectors, and activation functions.  We can denote the set of trainable parameters in our network by $\\theta = \\{ W^1, \\dots, W^L, b^1, \\dots, b^L \\}$.\n",
    "\n",
    "In this lab, we are only concerned with supervised learning tasks.  Recall that in supervised learning, we have a dataset represented by a list of $(x, y)$ pairs where $x$ is the input to our model and $y$ is the desired output.\n",
    "\n",
    "- For regression problems where we want to fit a function $y = f(x)$, $x$ is the independent variable vector, and $y$ is the function value.\n",
    "- In classification problems, $x$ will correspond to a set of attributes and $y$ the corresponding label.\n",
    "\n",
    "The goal of supervised learning is to \"train\" our network by adjusting its parameters in order to minimize a cost function over the entire training set,\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L} = \\min_\\theta \\sum_{i=1}^N \\ell^{(i)}\n",
    "$$\n",
    "\n",
    "where $\\ell^{(i)} = \\ell(\\sigma^L(\\sigma^{L-1}(\\dots(x^{(i)}W^1 + b^1)\\dots)W^L + b^L), y^{(i)})$ and $\\ell(\\hat{y}, y)$ denotes the particular form of the loss function being considered.  In this lab, we will use 2 different loss functions:\n",
    "\n",
    "1. Quadratic Loss: $\\ell(\\hat{y}, y) = \\|\\hat{y} - y\\|^2$\n",
    "2. Cross-entropy Loss: $\\ell(\\hat{y}, y) = -[ y \\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "f70A4AU__CzM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e69dd34bc58b78e3602de198b57680cd",
     "grade": false,
     "grade_id": "cell-6ac37293d56527cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Step 1: Build an untrainable network\n",
    "\n",
    "Understanding (and implementing) the back-propagation algorithm can seem a little daunting at first. Therefore, let's start by building out the functions we need just to create a network that cannot be trained, but can predict (feedforward pass). First load the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "PBkd3Edr_CzM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25cbd925d090df8bb8ae776550e82167",
     "grade": false,
     "grade_id": "cell-9925427eca70b36f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f11b1ac8115c056a001a8f917921fc8",
     "grade": false,
     "grade_id": "cell-1350577476da9ea6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We wish to implement activation functions ($\\sigma^\\ell$ in the notations above) as classes, to be able to also incorporate their derivatives (we will need them for back-propagation).\n",
    "\n",
    "However, we don't want multiple *instances* of activation function *objects* (the activation is always the same), hence we rely on *static* methods (these are \"Class-wide methods\" and do NOT depend on a particular object's values).\n",
    "\n",
    "Finally, we want to be able to call the activation function directly by its class' name, like a function.\n",
    "\n",
    "Due to all these reasons, we rely on the base class Activation below. In the next cell, we explain why it **appears** complicated (it's not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2b8a2c3057200fa3bace638a47a5dd7",
     "grade": false,
     "grade_id": "cell-10a3abf8585507c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*Python tricks:* \n",
    "- Static methods are callable with uninstanciated class objects.\n",
    "- `__call__` methods are retrieved when using parentheses with an instanciated class object.\n",
    "- However, instanciation has precedence over `__call__`, so calling a Class like another static method of that class or a function will not work (see [here](https://stackoverflow.com/questions/26793600/decorate-call-with-staticmethod)).\n",
    "- `__init__` (typically used to customize the instanciated objects of a class) cannot return anything.\n",
    "- The trick is thus to use it in conjunction with `__new__` (which creates the object and passes it to `__init__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4127da8445f0eb3468fb88d4c504aeac",
     "grade": false,
     "grade_id": "cell-7999382a4c9bf651",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"\n",
    "    Base activation node class\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        What to do when instantiating an object: this basically says Activation(x)\n",
    "        will be equivalent to Activation()(x).\n",
    "\n",
    "        :param float or np.array x: input of the activation\n",
    "        :return: call the activation function\n",
    "        :rtype: float or np.array\n",
    "        \"\"\"\n",
    "        return self.__call__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "YQCWlxli_CzQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fde720457e9061120bb56ec2f33c2dab",
     "grade": false,
     "grade_id": "cell-7ecb4be9dc4bc9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Implement the activation functions.\n",
    "\n",
    "1. The easiest activation we can implement is the identity function which simply returns the input as itself.  Implement this below in the class template `Identity`. The `prime` function should implement the derivative of the activation.\n",
    "\n",
    "*Python tricks*: `Identity` inherits from `Activation`; thus, calling `Identity(some_array)` will call `Identity.__new__(self, some_array)` which in turn will call the static method `Identity.__call__(some_array)`.\n",
    "\n",
    "*Python hint:* see [`np.ones_like`](https://numpy.org/doc/stable/reference/generated/numpy.ones_like.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd89ecb6b56e20c3e72f69144d2a005a",
     "grade": false,
     "grade_id": "cell-97b01d88950ba157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "    \"\"\"\n",
    "    Identity activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Implements the identity activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: identity of input\n",
    "        :rtype: float or np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @staticmethod\n",
    "    def prime(x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the identity activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de419a6f0bde69061daccfb05c4d78aa",
     "grade": true,
     "grade_id": "cell-60b6df8c945c757f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Identity(np.array([1])) == np.array([1])\n",
    "print(Identity(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "035e969b61f88e94f30f12845f6c62ec",
     "grade": false,
     "grade_id": "cell-a1eb8ef5a0c2aa25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. The threshold activation function takes an input and returns 1 if the input is positive, otherwise 0. Implement the `Threshold` class below.\n",
    "\n",
    "*Python hint:* see [`np.where`](https://numpy.org/doc/stable/reference/generated/numpy.where.html), and [`np.zeros_like`](https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5074f0d54387d09bb8909c32b065fd8",
     "grade": false,
     "grade_id": "cell-03e8948b196c4ddc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Threshold(Activation):\n",
    "    \"\"\"\n",
    "    Threshold activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Implements the threshold activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: if x > 0, 1, else 0\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def prime(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the threshold activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958793c7d923f70791c657ef7250fb5b",
     "grade": true,
     "grade_id": "cell-dbf41f4e682018a6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Threshold(np.array([0.5])) == np.array([1])\n",
    "print(Threshold(np.array([-0.1, 0.1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aadc87d1cfca836de19ebe4a626b3be3",
     "grade": false,
     "grade_id": "cell-f9a0356d080c3ad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Recall that the sigmoid function is given by $\\sigma(x) = \\dfrac{1}{1 + \\exp(-x)}$. Calculate its derivative.\n",
    "\n",
    "*Hint*: you can do this on paper but be sure to insert a new markdown cell below and then click Edit > Insert Image.\n",
    "\n",
    "*Recall* that the derivative of $\\dfrac{1}{f(x)}$ is $-\\dfrac{f'(x)}{f(x)^2}$. Rearrange the resulting expression so that only $\\sigma$ appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca1c7c3ec338e706eae6c3b93dd364b",
     "grade": true,
     "grade_id": "cell-fd3334fa63acd5fd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4295094b5635df5c64346df838963baa",
     "grade": false,
     "grade_id": "cell-b136c1c98c1b8aad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Implement the `Sigmoid` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5d181926f6f5c940f0492ea62b777d",
     "grade": false,
     "grade_id": "cell-b829708eeb7af423",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):    \n",
    "    \"\"\"\n",
    "    Threshold activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Implements the sigmoid activation function\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: sigmoid transform of input\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the sigmoid activation function\n",
    "        (you can make good use of Sigmoid(...)!)\n",
    "\n",
    "        :param np.array x: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1eee980947ed6aab9ab6e1c75c66c050",
     "grade": true,
     "grade_id": "cell-d141c4feaa9ed5dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Sigmoid(0) == 0.5\n",
    "print(Sigmoid(np.array([-1, 0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "xoN3Pbi__CzU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5642a74f64dff45fbf9d0b7156ef172",
     "grade": false,
     "grade_id": "cell-1e3ef5e338abc605",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Plot the activation functions and their derivative on the domain [-5, 5] with the function below. Are the graphical results satisfactory? (no answer expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "cNmcyS4Q_CzU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0f65d4ae0d72cf7f36a943a9c588c76",
     "grade": false,
     "grade_id": "cell-868d89397737e596",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_activation(activation: Activation):\n",
    "    x = np.arange(-5, 5, 0.01)\n",
    "    plt.plot(x, activation(x))\n",
    "    plt.plot(x, activation.prime(x))\n",
    "    plt.legend((activation.__name__, activation.__name__ + \" prime\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8f59408a1175f66919487ac6272deea",
     "grade": false,
     "grade_id": "cell-033f9a0310c31056",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(Identity)\n",
    "plot_activation(Threshold)\n",
    "plot_activation(Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "8alCasgu_CzZ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf54fc22c909cc667fa9a604128827cd",
     "grade": false,
     "grade_id": "cell-2b126872894e4635",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: First draft of the Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "uhCd0Nmr_CzW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf9ab616a1068d12ebb9e72481571cec",
     "grade": false,
     "grade_id": "cell-9cabe34a6f94db22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "During this lab, we will build on the following python class called `Network` which will represent our neural net.  The `Network` class will keep track of the weights, biases, and activations needed to evaluate and train our model.  The following exercises will guide you through building up the class from the skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "eY-JPx-3_CzX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ceb51b194d5b0bf62aaad0adc406f49",
     "grade": false,
     "grade_id": "cell-70244115571606c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    A simple implementation of a feed-forward artificial neural network.\n",
    "    Work on this class throughout the entire exercise, rerunning this cell after each update.\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes: list, sigmas: list):\n",
    "        \"\"\"\n",
    "        Construct a network given a list of the number of neurons in each layer and \n",
    "        a list of activations which will be applied to each layer.\n",
    "        \n",
    "        :param list sizes: A list of integers representing the number of nodes in each layer, \n",
    "            including the input and output layers.\n",
    "        :param list activations: A list of callable objects representing the activation functions.\n",
    "            Its size should be one less than sizes.\n",
    "        \"\"\"\n",
    "        # Exercise 2.2\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def num_params(self) -> int:\n",
    "        \"\"\"Returns the total number of trainable parameters in this network.\"\"\"\n",
    "        # Exercise 2.3\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def feed_forward(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Evaluates the network for the given input and returns the result.\n",
    "        \n",
    "        :param numpy.array x: A numpy 2D-array where the columns represent input variables\n",
    "            and rows represent independent samples.\n",
    "        :return: output of the network\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # Exercise 2.4\n",
    "        a = x  # we start with a^0 = x, the input\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a707892094291d7c688d0e360c19198a",
     "grade": false,
     "grade_id": "cell-3346888b14e4bb36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. From the recursion formulas above, write down the shapes of the weight matrices and bias vectors, given the number of neurons $d^l$ in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15df1237c9d043e47cdeca9ec273ea97",
     "grade": true,
     "grade_id": "cell-0821a88865b736d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ce02a5ea237f0e6bcd23af67261a47a",
     "grade": false,
     "grade_id": "cell-95e2712e5b603757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the init `__init__` function in `Network` above.\n",
    "\n",
    "* Store the provided `sizes` (a list of size $L + 1$ representing the respective size $d^l$ of each layer including the input which will be given as a parameter) and `sigmas` (a list of size $L$ of callable functions representing the activation function in each layer which will be given as a parameter) in attributes `sizes` and `sigmas`;\n",
    "\n",
    "* Construct a list of weight matrices (in a `weights` attribute), a list of bias vectors (in a `biases` attribute) which are randomly initialized from the standard normal distribution (see `np.random.randn`); pay attention to the size of each list and the shape of each element which must be consistent with `sizes`.\n",
    "\n",
    "*Python hints*: `array[1:]` gives you all the array except the first entry; `array[:-1]` gives you all the array except the last entry; `for x, y in zip(a, b)` lets you iterate \"simultaneously\" through a pair of objects of same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc05bd26ec2f132438104b043b262093",
     "grade": true,
     "grade_id": "cell-90370fca040d6c63",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's initialization\n",
    "assert Network([1], Sigmoid).sizes == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dca41e2071051f47e070681b342c8844",
     "grade": false,
     "grade_id": "cell-1abf1a64a6747dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Implement the `num_params` function in `Network` above.  Use your knowledge of the shapes of the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3850aee0ed3652348b4027d689ee4a4d",
     "grade": true,
     "grade_id": "cell-b870d6b966b394b7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's number of parameters\n",
    "assert Network([1, 2, 2, 1], [Sigmoid, Sigmoid, Sigmoid]).num_params() == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b27641ef5f488839f2e924946d119197",
     "grade": false,
     "grade_id": "cell-ebb67909ec605029",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Implement the `feed_forward` function in `Network` above. Use the recurrence relations discussed in the Notation section, in particular:\n",
    "\n",
    "$$z^l = a^{l-1} W^l + b^l,\\\\\n",
    "a^l = \\sigma^l(z^l).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ebb3241e7515db6fbd566d77930bef",
     "grade": true,
     "grade_id": "cell-f18884d1944a1976",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the network's feedforward result\n",
    "assert Network([1, 2, 2, 1], [Sigmoid, Sigmoid, Sigmoid]).feed_forward(np.array([1])).shape[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "gSel8dO__Cza",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9167eb94427a79de589c98abd14f64",
     "grade": false,
     "grade_id": "cell-4bddc8c9c4cbed49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Create a `network` with 4 inputs, 2 hidden layers of 5 nodes each, and 1 output (use any activation). This network could typically be used with the `iris` dataset, since we have 4 features (petal/sepal length/width) and 1 output (setosa/versicolor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "11dZmuk5_Czb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63c192afcd5aba6cd9490ef93c106420",
     "grade": false,
     "grade_id": "cell-f4d5798cb8807a54",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# network = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "O1HGWxSg_Czh",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89021dc6bb2238772979e2b52489ec62",
     "grade": false,
     "grade_id": "cell-99327dd1116cd912",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Print the weights and biases of the network and confirm they are intialized correctly (shape and values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "ri_SV2x0_Czi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0e1de9ecc086523fdf14ea65a0f5797",
     "grade": true,
     "grade_id": "cell-98afd0b9734c77a8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for index, weight in enumerate(network.weights):\n",
    "    print(f\"\\nW^{index}\")\n",
    "    print(\"Shape:\", weight.shape)\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40d2cbcabe422f44780b364551037aa3",
     "grade": true,
     "grade_id": "cell-034bea04084ae05d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for index, bias in enumerate(network.biases):\n",
    "    print(\"\\nBias\", index)\n",
    "    print(\"Shape:\", bias.shape)\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "VXTC5Gpe_Czm",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "341d2e6ed9b0f813c20ad3c25c38ba0c",
     "grade": false,
     "grade_id": "cell-9285ed448929d454",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Build logic gates\n",
    "\n",
    "Below is a table of logical functions (logic gates).  Each function takes two values (A and B) representing True (1) or False (0) propositions and returns a True or False value.\n",
    "\n",
    "| A | B | AND | OR | XOR | NAND |\n",
    "|---|---|-----|----|-----|------|\n",
    "| 0 | 0 | 0   | 0  | 0   | 1    |\n",
    "| 0 | 1 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 0 | 0   | 1  | 1   | 1    |\n",
    "| 1 | 1 | 1   | 1  | 0   | 0    |\n",
    "\n",
    "Interestingly, [it is possible to create any boolean function of any size through a combination of NAND gates](https://en.wikipedia.org/wiki/NAND_gate)!  Thus, if we can create a network which reproduces the logic behind a NAND-gate, it is possible to represent any logical function (and by extension any mathematical function) by combining such a network into ever more complex networks.  This is one version of a universal approximation theorem for ANNs.\n",
    "\n",
    "**Task:** build the AND, OR, and NAND logic gates above using the simple network below by **modifying its weights and biases directly**. The `logic_gate` function is provided to test your networks: it corresponds to columns A, B in the table above. We will test if we get the expected results (columns AND, ..., NAND)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "992e3cf96e2af53cb70a4e4c2857b71c",
     "grade": false,
     "grade_id": "cell-3718598f00e4622d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logic_gate(network: Network, array: np.array = np.asarray([\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])) -> np.array:\n",
    "    \"\"\"\n",
    "    Helper function to test our network as a logic gate.\n",
    "\n",
    "    :param Network network: our neural network\n",
    "    :param numpy.array array: apply feed forward to array\n",
    "    :return: output of network given array\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    return network.feed_forward(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79f5ef1ddb2873f7fe2ee2b974a546ce",
     "grade": false,
     "grade_id": "cell-155227b9348f53c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple \"logic gate\" network with two inputs A and B and 1 output, with activation function Threshold\n",
    "network = Network([2, 1], [Threshold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06a272ffb2d4ae6825d91171fbce12a6",
     "grade": false,
     "grade_id": "cell-b6d135b9536dd76e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Implement the AND gate by modifying the `network`'s weights and biases.\n",
    "\n",
    "*Hint:* what should $W$ and $b$ be such that we get `[0, 0, 0, 1]` when the input is (A, B)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Qp7utpCB_Czn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37ce797e6676a9b2386a3b1a19204e5",
     "grade": false,
     "grade_id": "cell-a42131ad5e265fef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AND gate\n",
    "# network.weights[0] =  ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ee905c885fd0881cff13de556e0ce11",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([0, 0, 0, 1]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f49f86c3d452925debbd60472229c8a",
     "grade": false,
     "grade_id": "cell-4725d6fb651243c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the OR gate by modifying the `network`'s weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61180b45dd694b32598d974a6c64e73a",
     "grade": false,
     "grade_id": "cell-480fe471b4467d26",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# OR gate\n",
    "# network.weights[0] = ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa65a7636e93a5543e94402b3b87ac04",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([0, 1, 1, 1]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87bda956c6842571606f991c2be92f7c",
     "grade": false,
     "grade_id": "cell-06c4fc60e02fda82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Implement the NAND gate by modifying the `network`'s weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "581f3b4a3ce0f07f4f510f8ec82f8405",
     "grade": false,
     "grade_id": "cell-bfa280b436bf2d7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NAND gate\n",
    "# network.weights[0] = ...\n",
    "# network.biases[0] = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57546bf42555b02ac2810b6a4ba2bd89",
     "grade": true,
     "grade_id": "cell-846b9303cb272ebe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if we get the expected result (see table above)\n",
    "np.testing.assert_array_equal(logic_gate(network), np.array([1, 1, 1, 0]).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "101fc96163abfb1a6c462e49f16314b1",
     "grade": false,
     "grade_id": "cell-144539a8a6ee7b5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Testing your Network's Capacity\n",
    "\n",
    "You haven't trained one of your networks yet. But the theorem of universal approximation doesn't say anything about training regimes. A network may still possess enormous capacity. \n",
    "\n",
    "Every so often in the scientific literature the idea resurfaces of harnessing the power *of an untrained network*. Sometimes such methodologies are given creative and exhuberent names such as \"Random Basis Function Expansion\", \"Extreme Learning Machines\", \"Echo State Networks\" or \"Random Kitchen Sinks\". The idea is simple, but ingenious: \n",
    "1. Use a neural network to create a deep random projection into a high dimensional space - this is done by having many hidden layers with (at least) a big output layer ($d^L >> 1$);\n",
    "1. Train least squares or similar using the projection as input\n",
    "1. Aggressively regularize the network. No deep learning/back-propagation required!\n",
    "\n",
    "Observe the following non-linear function; notice how it cannot be solved by least squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03cf953bdd8160675f218c989b021f76",
     "grade": false,
     "grade_id": "cell-65078b8c10006094",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    \"\"\"put x into non linear 2D space\"\"\"\n",
    "    return np.column_stack([np.sin(x), np.sqrt(x)])\n",
    "\n",
    "def f_true(phi):\n",
    "    \"\"\"linear regression in that non linear space\"\"\"\n",
    "    return phi @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e3fd8d17b2f5fc680e6a9f077a9259",
     "grade": false,
     "grade_id": "cell-904865f1b6d2c7ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# linear regression coefficient in non linear space\n",
    "w = np.array([0.8, 1.2]).reshape(2, 1)\n",
    "\n",
    "# generate dataset\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1)  # to get a column vector\n",
    "y = f_true(phi(x)) + 0  # no noise for this demonstration, we just want to fit...\n",
    "\n",
    "# baseline: a linear regression on x (with intercept)\n",
    "fhat = linear_model.LinearRegression()\n",
    "fhat.fit(x, y)\n",
    "yp = fhat.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b318407c1f840d90b1bea5b2cbd6b8a",
     "grade": false,
     "grade_id": "cell-d500b1ef0ac164f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot truth and estimate\n",
    "fig = plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f_true(phi(x)), label=\"$f_\\star$\")\n",
    "plt.plot(x, yp, label=\"$\\hat f$ (linear regression)\")\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6f08efd925736aa7ae4b8a299a49a97",
     "grade": false,
     "grade_id": "cell-d6f47c07fcaa9b37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, create an instance of your network to carry out step 1. by having it produce an output of size `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "831ccc82eb5c655a2e44ead333151252",
     "grade": false,
     "grade_id": "cell-3f05485b83e8b21d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 40\n",
    "\n",
    "# TODO: Use your network to convert each x[i] into a random k-dimensional projection, Z[i] \n",
    "# Hint 1: Try with sizes [1, int(k/2), k]\n",
    "# Hint 2: If you stick to linear activation functions, the result can only be linear!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "Z = network.feed_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b6ddec63089c845c596e7c7478bfa7d",
     "grade": true,
     "grade_id": "cell-cf5b31928a0861c7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert Z.shape == (x.shape[0], k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "714e4e6d660fc39543a0e40e156e87c6",
     "grade": false,
     "grade_id": "cell-3dfae7a38eec2d6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, use a linear regression (see \"baseline\" above) that uses the output of your \"random\" network as an input and tries to predict `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8b94c8e9dbec035392fb2505be7c001",
     "grade": false,
     "grade_id": "cell-d8fe7b0c4fd6906e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# yp = ...  # <= to UNCOMMENT AND COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae111f709cd46ab9021ff1d264011f0",
     "grade": true,
     "grade_id": "cell-1fb6d0e684b62fbf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert yp.shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "137ed29a4826195583bb9fbc49dd7a86",
     "grade": false,
     "grade_id": "cell-488517d32c18a0a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check graphically that the estimate fits the truth very well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aef1370b43e72fb8318faa04c0f20658",
     "grade": false,
     "grade_id": "cell-dd40d0d47eaa68c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, f_true(phi(x)), label=\"$f_\\star$\")\n",
    "plt.plot(x, yp, label=\"$\\hat f$ (ELM)\")\n",
    "plt.legend()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ac2928908464eaacbab57ef09c9f232",
     "grade": true,
     "grade_id": "cell-02d2221fc8db0dab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de95556c4633f457216e41e4bf8f3270",
     "grade": false,
     "grade_id": "cell-a92106751bcd763e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(Note: Step 3.-Regularization is of enormous importance (as you know by now), but in this exercise we are particularly interested in confirming the capacity/power of your network.)\n",
    "\n",
    "Question/discussion point: Why don't we always use this technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "yiF9J9OA_Czr",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a75d877448c69b3d95cccb05fbe15b4d",
     "grade": false,
     "grade_id": "cell-61601a225ff5885a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Training the network\n",
    "\n",
    "While interesting, the `Network` class above is pretty useless as it stands since there is no way to learn a function we don't already know.  In this step, we will add the ability to train our network on a dataset using the gradient descent and back-propagation algorithms.  Let's review both of these algorithms now.\n",
    "\n",
    "### (Stochastic) Gradient Descent\n",
    "\n",
    "Recall from the beginning of the lab that we want to train the network parameters by minimizing a given loss function over an entire dataset. One method of doing this is using the gradient descent (GD) algorithm which you have already seen in the lectures and previous labs; recall the update rule:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla_\\theta \\mathcal{L},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.  The update rule above is called GD because direction of change in the network parameters follow the opposite of the parameters' gradient in the loss function.  You can think of this like a ball rolling down a hill to find the minimum of the topology. Only in this case, the ball is massless because it has no momentum. Note that, contrary to linear and logistic regression, the loss / error function might not be convex, hence the existence of a unique minimizer isn't guaranteed, nor is it guaranteed that gradient descent will find it.\n",
    "\n",
    "When the update rule is applied to a random subset of the total dataset, it is called stochastic gradient descent (SGD). SGD is far more efficient (computationally) than GD when the batch size is large enough to approximate the true gradients while being significantly smaller than the full dataset. Running over the entire dataset with SGD once is called an \"epoch\" (of training).\n",
    "\n",
    "### Back-Propagation\n",
    "\n",
    "From the GD update rule above, it is clear we will need to compute the gradients of the network parameters with respect to the cost function. This is exactly what back-propagation does, and thus is a crucial component to almost all neural network learning algorithms. In the next exercise, we will derive the 4 equations in back-propagation. You will need knowledge of the [chain rule for differentiation](https://en.wikipedia.org/wiki/Chain_rule) if you are not already familiar with this.\n",
    "\n",
    "### Exercise 5: Derive backprop formulas\n",
    "\n",
    "Before we start, it is convenient to define the following variable:\n",
    "\n",
    "$$\n",
    "\\delta^l \\equiv \\frac{\\partial \\ell_p}{\\partial z^l}.\n",
    "$$\n",
    "\n",
    "In other words, $\\delta^l$ is the gradient of the loss function for a point $p$ with respect to the input to the activation function for the layer $l$ in our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0499d5a0056e7853f8c36be38a0652ce",
     "grade": false,
     "grade_id": "cell-9a2cffdf102ab517",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. What is the shape of $\\delta^L$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3c2180f3932dc699f445df1f7481df2",
     "grade": true,
     "grade_id": "cell-865769c85dc2a9d9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69d72d5b65158dae365276cb97af1c8f",
     "grade": false,
     "grade_id": "cell-0858c5f882d20f8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Show that $\\delta^L = \\nabla_{a^L} \\ell_p  \\odot {\\sigma'}^L ( z^L )$.\n",
    "\n",
    "*Hint:* apply the chain rule to the definition of $\\delta^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21fee3ea20b5bdc41787027a873436d7",
     "grade": true,
     "grade_id": "cell-c6710eca7f38cfd4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd6a163a7815888a7f7f8dcd15a2652a",
     "grade": false,
     "grade_id": "cell-1535ada7abc45b49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Show that for $l < L$, $\\delta^l = [\\delta^{l+1} (W^{l+1})^T ] \\odot {\\sigma'}^l ( z^l )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "412cd8077510fb9c417f756175727513",
     "grade": true,
     "grade_id": "cell-727fb651f4009ba5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3fd692815dc5549ed95912515871c4c",
     "grade": false,
     "grade_id": "cell-8feb9a660f56fe91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Show that $\\nabla_{W^l}\\ell_p = (a^{l-1})^T \\delta^l$.\n",
    "\n",
    "*Hint:* use the definition of $z^l$ and derive w.r.t. a single component $W_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e1832939317e82bfdc448ebaa1bb86f",
     "grade": true,
     "grade_id": "cell-4f2e22170c5f182e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f73f96c72665a5c9ddfd937e3ce6ad19",
     "grade": false,
     "grade_id": "cell-3e26d4c6bbded646",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Show that $\\nabla_{b^l}\\ell_p = \\delta^l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a579082f8e87d1b5bc1ad1bb37b4e4a2",
     "grade": true,
     "grade_id": "cell-1612d709394748c4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6f641bfa8fd6f1a84ae5ef9848fe7c4",
     "grade": false,
     "grade_id": "cell-77d5ea6f4ae46501",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that these four last formulas allow us to compute the gradient of the loss function for a single training point with respect to the parameters of our network.  Looking at the equations more closely, you should see a possible algorithm form.\n",
    "\n",
    "1. Compute the values of $z^l$ and $a^l$ for $l = {1, \\dots, L}$ by forward propagation through the network.  Recall $a^0 = x_p$: that's what we've implemented throughout Exercises 1-4.\n",
    "2. Use the equation in Exercise 5.2 above to compute $\\delta^L$.  \n",
    "3. Back-propagate in the reverse direction using the equation in Exercise 5.3 to get all the other $\\delta^l$ values.\n",
    "4. Compute the parameter gradients using the equations in Exercises 5.4 and 5.5.\n",
    "\n",
    "Note that this will yield the gradients for a single training point.  The gradients for the total loss function can easily be computed by\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L} = \\sum_{p=1}^N \\nabla_\\theta \\ell_p.\n",
    "$$\n",
    "\n",
    "Of course, the exact form of $\\nabla_\\theta \\ell_p$ will depend on the choice of $\\ell$.\n",
    "\n",
    "We will implement this algorithm next week."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copie de Lab6.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab6.ipynb",
     "timestamp": 1576227403767
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
